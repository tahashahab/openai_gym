{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook all algorithms that I reference are from the textbook: Reinforcement Learning: An Introduction by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/wiki/CartPole-v0\n",
    "\n",
    "This link has all the information regarding the specifics of the CartPole environment, such as the action space, how rewards are disitributed, and what it means to solve this problem.\n",
    "\n",
    "Solved requirements: \"Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "We will first attempt to solve this problem with the Q-learning algorithm, which is an off-policy temporal difference control algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment with openai gym and set a seed to work in\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6, 12, 6 , 12)\n",
    "lower_bounds = [-4.8, env.observation_space.low[1], env.observation_space.low[2], -42]\n",
    "upper_bounds = [4.8, env.observation_space.high[1], env.observation_space.high[2], 42]\n",
    "\n",
    "def discretizer(pos, cart_velocity, angle, pole_velocity):\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, cart_velocity, angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 6, 12, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn():\n",
    "    tic = time.time()\n",
    "    Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "    reward_list = []\n",
    "    total_reward = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            action = np.argmax(Q_table[current])\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "            current = next_state\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "            reward_list.append(total_reward/100)\n",
    "#            print(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= 195.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 55.99 seconds and failed\n",
      "Max average reward achieved: 19.89\n"
     ]
    }
   ],
   "source": [
    "qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment with openai gym and set a seed to work in\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    tic = time.time()\n",
    "    reward_list = []\n",
    "    Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "    policy = np.zeros(n_bins)\n",
    "    total_reward = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            action = int(policy[current])\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][action] += lr*(reward + Q_table[next_state][int(policy[next_state])] - Q_table[current][action])\n",
    "            current = next_state\n",
    "            if np.random.random() < epsilon:\n",
    "                policy[current] = env.action_space.sample()\n",
    "            else:\n",
    "                policy[current] = np.argmax(Q_table[current])\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "#            print(total_reward/100)\n",
    "            reward_list.append(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= 195.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 55.41 seconds and failed\n",
      "Max average reward achieved: 19.56\n"
     ]
    }
   ],
   "source": [
    "sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
