{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook all algorithms that I reference are from the textbook: Reinforcement Learning: An Introduction by Sutton and Barto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gym/wiki/CartPole-v0\n",
    "\n",
    "This link has all the information regarding the specifics of the CartPole environment, such as the action space, how rewards are disitributed, and what it means to solve this problem.\n",
    "\n",
    "Solved requirements: \"Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "We will first attempt to solve this problem with the Q-learning algorithm, which is an off-policy temporal difference control algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the environment with openai gym and set a seed to work in\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6, 12, 6 , 12)\n",
    "lower_bounds = [-4.8, env.observation_space.low[1], env.observation_space.low[2], -42]\n",
    "upper_bounds = [4.8, env.observation_space.high[1], env.observation_space.high[2], 42]\n",
    "\n",
    "def discretizer(pos, cart_velocity, angle, pole_velocity):\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, cart_velocity, angle, pole_velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 6, 12, 2)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn():\n",
    "    tic = time.time()\n",
    "    Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "    reward_list = []\n",
    "    total_reward = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            action = np.argmax(Q_table[current])\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "            current = next_state\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "            reward_list.append(total_reward/100)\n",
    "            print(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= 195.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 54.04 seconds and failed\n",
      "Max average reward achieved: 22.71\n"
     ]
    }
   ],
   "source": [
    "qlearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-085dc890695a>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-68-085dc890695a>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    next_state = discretizer(*observation)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def sarsa():\n",
    "    tic = time.time()\n",
    "    reward_list = []\n",
    "    Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "    policy = np.zeros(n_bins)\n",
    "    total_reward = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            observation, reward, done, info = env.step(int(policy[current])\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][int(policy[current])] += lr*(reward + Q_table[next_state][int(policy[next_state])] - Q_table[current][int([policy[current]])])\n",
    "            current = next_state\n",
    "            if np.random.random() < epsilon:\n",
    "                policy[current] = env.action_space.sample()\n",
    "            else:\n",
    "                policy[current] = np.argmax(Q_table[current])\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "            print(total_reward)\n",
    "            reward_list.append(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= 195.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5ba5a22bafde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msarsa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-0f9652c3cc93>\u001b[0m in \u001b[0;36msarsa\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12, 6, 12)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros(n_bins)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(p[discretizer(*env.reset())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
