{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MountainCarEnv\n",
       "\u001b[1;31mString form:\u001b[0m <MountainCarEnv<MountainCar-v0>>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\taha\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    The agent (a car) is started at the bottom of a valley. For any given\n",
       "    state the agent may choose to accelerate to the left, right or cease\n",
       "    any acceleration.\n",
       "\n",
       "Source:\n",
       "    The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
       "\n",
       "Observation:\n",
       "    Type: Box(2)\n",
       "    Num    Observation               Min            Max\n",
       "    0      Car Position              -1.2           0.6\n",
       "    1      Car Velocity              -0.07          0.07\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(3)\n",
       "    Num    Action\n",
       "    0      Accelerate to the Left\n",
       "    1      Don't accelerate\n",
       "    2      Accelerate to the Right\n",
       "\n",
       "    Note: This does not affect the amount of velocity affected by the\n",
       "    gravitational pull acting on the car.\n",
       "\n",
       "Reward:\n",
       "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
       "     on top of the mountain.\n",
       "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
       "\n",
       "Starting State:\n",
       "     The position of the car is assigned a uniform random value in\n",
       "     [-0.6 , -0.4].\n",
       "     The starting velocity of the car is always assigned to 0.\n",
       "\n",
       "Episode Termination:\n",
       "     The car position is more than 0.5\n",
       "     Episode length is greater than 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (10,10)\n",
    "lower_bounds = [-1.2, -1.5]\n",
    "upper_bounds = [0.5, 1.5]\n",
    "\n",
    "def discretizer(pos, velocity):\n",
    "    est = KBinsDiscretizer(n_bins=state_space, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-477e8d73f900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscretizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-943a0e1fe7fe>\u001b[0m in \u001b[0;36mdiscretizer\u001b[1;34m(pos, velocity)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKBinsDiscretizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_bins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ordinal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uniform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlower_bounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupper_bounds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvelocity\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0matol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.e-8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m             \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrtol\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m             \u001b[0mXt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_edges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_bins_\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "reward_list = []\n",
    "total_reward = 0\n",
    "success = False\n",
    "for i in range(1, 20000):\n",
    "    current, done = discretizer(*env.reset()), False\n",
    "    while not done:\n",
    "        lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "        epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current])\n",
    "        observation, reward, done, info = env.step(action) \n",
    "        next_state = discretizer(*observation)\n",
    "        Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "        current = next_state\n",
    "        total_reward += reward\n",
    "#        if i % 100:\n",
    "#            env.render()\n",
    "    if i % 100 == 0:\n",
    "#        print(total_reward/100)\n",
    "        reward_list.append(total_reward/100)\n",
    "        if reward_list[-1] >= -110.0:\n",
    "            toc = time.time()\n",
    "            success = True\n",
    "            print(f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[-1]} in {(reward_list.index(reward_list[-1]) + 1)*100} episodes')\n",
    "            env.close()\n",
    "            break\n",
    "        total_reward = 0\n",
    "toc = time.time()\n",
    "if not success:\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "env.close()\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
