{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MountainCarEnv\n",
       "\u001b[1;31mString form:\u001b[0m <MountainCarEnv<MountainCar-v0>>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\taha\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    The agent (a car) is started at the bottom of a valley. For any given\n",
       "    state the agent may choose to accelerate to the left, right or cease\n",
       "    any acceleration.\n",
       "\n",
       "Source:\n",
       "    The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
       "\n",
       "Observation:\n",
       "    Type: Box(2)\n",
       "    Num    Observation               Min            Max\n",
       "    0      Car Position              -1.2           0.6\n",
       "    1      Car Velocity              -0.07          0.07\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(3)\n",
       "    Num    Action\n",
       "    0      Accelerate to the Left\n",
       "    1      Don't accelerate\n",
       "    2      Accelerate to the Right\n",
       "\n",
       "    Note: This does not affect the amount of velocity affected by the\n",
       "    gravitational pull acting on the car.\n",
       "\n",
       "Reward:\n",
       "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
       "     on top of the mountain.\n",
       "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
       "\n",
       "Starting State:\n",
       "     The position of the car is assigned a uniform random value in\n",
       "     [-0.6 , -0.4].\n",
       "     The starting velocity of the car is always assigned to 0.\n",
       "\n",
       "Episode Termination:\n",
       "     The car position is more than 0.5\n",
       "     Episode length is greater than 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6 , 6)\n",
    "lower_bounds = [env.observation_space.low[0], env.observation_space.low[1]]\n",
    "upper_bounds = [env.observation_space.high[0], env.observation_space.high[1]]\n",
    "\n",
    "def discretizer(pos, velocity):\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn():\n",
    "    tic = time.time()\n",
    "    reward_list = []\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            action = np.argmax(Q_table[current])\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "            current = next_state\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "            t += 1\n",
    "            reward_list.append(total_reward/100)\n",
    "#            print(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= -110.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-200.0\n",
      "-200.0\n",
      "-170.62\n",
      "-177.32\n",
      "-151.92\n",
      "-170.71\n",
      "-144.03\n",
      "-132.51\n",
      "-138.14\n",
      "-149.5\n",
      "-196.9\n",
      "-194.8\n",
      "-196.03\n",
      "-192.96\n",
      "-187.46\n",
      "-183.84\n",
      "-187.18\n",
      "-175.43\n",
      "-195.2\n",
      "-186.37\n",
      "-160.43\n",
      "-168.88\n",
      "-188.26\n",
      "-191.59\n",
      "-189.8\n",
      "-186.08\n",
      "-183.37\n",
      "-164.19\n",
      "-192.61\n",
      "-187.9\n",
      "-197.63\n",
      "-164.69\n",
      "-169.04\n",
      "-175.54\n",
      "-170.56\n",
      "-179.31\n",
      "-167.79\n",
      "-171.45\n",
      "-194.97\n",
      "-153.43\n",
      "-179.73\n",
      "-188.54\n",
      "-194.28\n",
      "-189.71\n",
      "-196.81\n",
      "-181.22\n",
      "-190.76\n",
      "-174.56\n",
      "-169.59\n",
      "-185.74\n",
      "-195.42\n",
      "-196.33\n",
      "-198.09\n",
      "-191.2\n",
      "-194.61\n",
      "-194.13\n",
      "-195.5\n",
      "-193.72\n",
      "-188.98\n",
      "-194.4\n",
      "-190.9\n",
      "-192.01\n",
      "-185.76\n",
      "-175.75\n",
      "-161.42\n",
      "-171.63\n",
      "-196.99\n",
      "-173.42\n",
      "-159.42\n",
      "-166.54\n",
      "-165.15\n",
      "-169.75\n",
      "-163.89\n",
      "-168.03\n",
      "-190.31\n",
      "-185.21\n",
      "-177.65\n",
      "-189.4\n",
      "-195.49\n",
      "-196.7\n",
      "-171.09\n",
      "-197.48\n",
      "-194.3\n",
      "-196.84\n",
      "-178.09\n",
      "-173.01\n",
      "-185.75\n",
      "-199.59\n",
      "-193.91\n",
      "-175.04\n",
      "-186.74\n",
      "-175.12\n",
      "-169.13\n",
      "-174.74\n",
      "-181.07\n",
      "-180.85\n",
      "-192.68\n",
      "-190.85\n",
      "-196.84\n",
      "-190.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-132.51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
