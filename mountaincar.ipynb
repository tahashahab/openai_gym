{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MountainCarEnv\n",
       "\u001b[1;31mString form:\u001b[0m <MountainCarEnv<MountainCar-v0>>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\taha\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    The agent (a car) is started at the bottom of a valley. For any given\n",
       "    state the agent may choose to accelerate to the left, right or cease\n",
       "    any acceleration.\n",
       "\n",
       "Source:\n",
       "    The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
       "\n",
       "Observation:\n",
       "    Type: Box(2)\n",
       "    Num    Observation               Min            Max\n",
       "    0      Car Position              -1.2           0.6\n",
       "    1      Car Velocity              -0.07          0.07\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(3)\n",
       "    Num    Action\n",
       "    0      Accelerate to the Left\n",
       "    1      Don't accelerate\n",
       "    2      Accelerate to the Right\n",
       "\n",
       "    Note: This does not affect the amount of velocity affected by the\n",
       "    gravitational pull acting on the car.\n",
       "\n",
       "Reward:\n",
       "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
       "     on top of the mountain.\n",
       "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
       "\n",
       "Starting State:\n",
       "     The position of the car is assigned a uniform random value in\n",
       "     [-0.6 , -0.4].\n",
       "     The starting velocity of the car is always assigned to 0.\n",
       "\n",
       "Episode Termination:\n",
       "     The car position is more than 0.5\n",
       "     Episode length is greater than 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (10,10)\n",
    "lower_bounds = [-1.2, -1.5]\n",
    "upper_bounds = [0.6, 1.5]\n",
    "\n",
    "def discretizer(pos, velocity):\n",
    "    est = KBinsDiscretizer(n_bins=state_space, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1528.58 seconds and failed\n",
      "Max average reward achieved: -158.60\n",
      "[[[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-350.01142788 -350.02397224 -350.00641633]\n",
      "  [-350.48803978 -350.49554891 -350.36860047]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-351.73969794 -351.78366714 -351.78746713]\n",
      "  [-347.65383548 -346.46173148 -347.6229646 ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-352.99480966 -352.94157763 -352.25685141]\n",
      "  [-345.61989315 -345.00492051 -340.07943423]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-358.67302432 -358.54624041 -357.11862755]\n",
      "  [-346.32283661 -343.21883939 -334.8350029 ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-363.90568259 -364.28466789 -364.49563112]\n",
      "  [-334.83875355 -332.32011783 -329.1688415 ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-368.59332621 -368.58932381 -368.5420274 ]\n",
      "  [-316.53460932 -316.6166419  -316.14994753]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-370.38228836 -370.39804236 -370.39173855]\n",
      "  [-295.61600445 -295.68195152 -294.58895409]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-369.02016147 -369.02764832 -369.04203572]\n",
      "  [-258.78045944 -258.75642737 -254.41624558]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-331.64719258 -331.28974521 -331.57101259]\n",
      "  [-155.03215348 -154.34592824 -149.02639488]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [ -94.41486481  -94.08043849  -93.93770724]\n",
      "  [ -92.78223743  -92.78022809  -92.77624672]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "reward_list = []\n",
    "total_reward = 0\n",
    "success = False\n",
    "for i in range(1, 20000):\n",
    "    current, done = discretizer(*env.reset()), False\n",
    "    while not done:\n",
    "        lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "        epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current])\n",
    "        observation, reward, done, info = env.step(action) \n",
    "        next_state = discretizer(*observation)\n",
    "        Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "        current = next_state\n",
    "        total_reward += reward\n",
    "    if i % 100 == 0:\n",
    "#        print(total_reward/100)\n",
    "        reward_list.append(total_reward/100)\n",
    "        if reward_list[-1] >= -110.0:\n",
    "            toc = time.time()\n",
    "            success = True\n",
    "            print(f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[-1]} in {(reward_list.index(reward_list[-1]) + 1)*100} episodes')\n",
    "            env.close()\n",
    "            break\n",
    "        total_reward = 0\n",
    "toc = time.time()\n",
    "if not success:\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "env.close()\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
