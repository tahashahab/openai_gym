{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = (10,10)\n",
    "lower_bounds = [-1.2, -1.5]\n",
    "upper_bounds = [0.5, 1.5]\n",
    "\n",
    "def discretizer(pos, velocity):\n",
    "    est = KBinsDiscretizer(n_bins=state_space, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1557.76 seconds and failed\n",
      "Max average reward achieved: -164.75\n",
      "[[[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-269.93338748 -265.24500577 -270.90371561]\n",
      "  [-262.64262517 -261.15046072 -259.69499683]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-267.67507978 -266.53499923 -268.02557636]\n",
      "  [-255.09858408 -255.0708204  -255.09637147]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-266.84286689 -266.84426854 -266.57168087]\n",
      "  [-252.24161009 -252.21164125 -252.00655643]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-931.84756015 -931.83789838 -931.84122981]\n",
      "  [-930.51252576 -930.50422781 -930.5069949 ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-930.87052055 -930.86605671 -930.88816751]\n",
      "  [-930.61258638 -930.60578093 -930.60995055]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-281.98653465 -284.46088172 -284.22935599]\n",
      "  [-246.7731964  -247.00780805 -246.94563038]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-291.98654501 -287.31963855 -291.72000119]\n",
      "  [-233.80927707 -234.19692586 -234.10687837]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-292.46626773 -298.56663444 -298.67393036]\n",
      "  [-210.37034408 -208.69887641 -210.48102523]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-294.94776308 -294.95397914 -294.9302301 ]\n",
      "  [-164.26240443 -164.69597221 -163.1212817 ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]\n",
      "\n",
      " [[   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [-160.23475549 -161.00220272 -159.1893009 ]\n",
      "  [-124.7764454  -124.75993684 -124.63881042]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "Q_table = np.zeros(state_space + (env.action_space.n,))\n",
    "reward_list = []\n",
    "total_reward = 0\n",
    "success = False\n",
    "for i in range(1, 20000):\n",
    "    current, done = discretizer(*env.reset()), False\n",
    "    while not done:\n",
    "        lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "        epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current])\n",
    "        observation, reward, done, info = env.step(action) \n",
    "        next_state = discretizer(*observation)\n",
    "        Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "        current = next_state\n",
    "        total_reward += reward\n",
    "#        if i % 100:\n",
    "#            env.render()\n",
    "    if i % 100 == 0:\n",
    "#        print(total_reward/100)\n",
    "        reward_list.append(total_reward/100)\n",
    "        if reward_list[-1] >= -110.0:\n",
    "            toc = time.time()\n",
    "            success = True\n",
    "            print(f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[-1]} in {(reward_list.index(reward_list[-1]) + 1)*100} episodes')\n",
    "            env.close()\n",
    "            break\n",
    "        total_reward = 0\n",
    "toc = time.time()\n",
    "if not success:\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "env.close()\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
