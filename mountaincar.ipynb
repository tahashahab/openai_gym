{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        MountainCarEnv\n",
       "\u001b[1;31mString form:\u001b[0m <MountainCarEnv<MountainCar-v0>>\n",
       "\u001b[1;31mFile:\u001b[0m        c:\\users\\taha\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\mountain_car.py\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Description:\n",
       "    The agent (a car) is started at the bottom of a valley. For any given\n",
       "    state the agent may choose to accelerate to the left, right or cease\n",
       "    any acceleration.\n",
       "\n",
       "Source:\n",
       "    The environment appeared first in Andrew Moore's PhD Thesis (1990).\n",
       "\n",
       "Observation:\n",
       "    Type: Box(2)\n",
       "    Num    Observation               Min            Max\n",
       "    0      Car Position              -1.2           0.6\n",
       "    1      Car Velocity              -0.07          0.07\n",
       "\n",
       "Actions:\n",
       "    Type: Discrete(3)\n",
       "    Num    Action\n",
       "    0      Accelerate to the Left\n",
       "    1      Don't accelerate\n",
       "    2      Accelerate to the Right\n",
       "\n",
       "    Note: This does not affect the amount of velocity affected by the\n",
       "    gravitational pull acting on the car.\n",
       "\n",
       "Reward:\n",
       "     Reward of 0 is awarded if the agent reached the flag (position = 0.5)\n",
       "     on top of the mountain.\n",
       "     Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
       "\n",
       "Starting State:\n",
       "     The position of the car is assigned a uniform random value in\n",
       "     [-0.6 , -0.4].\n",
       "     The starting velocity of the car is always assigned to 0.\n",
       "\n",
       "Episode Termination:\n",
       "     The car position is more than 0.5\n",
       "     Episode length is greater than 200\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#?env.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = (6 , 6)\n",
    "lower_bounds = [env.observation_space.low[0], env.observation_space.low[1]]\n",
    "upper_bounds = [env.observation_space.high[0], env.observation_space.high[1]]\n",
    "\n",
    "def discretizer(pos, velocity):\n",
    "    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='uniform')\n",
    "    est.fit([lower_bounds, upper_bounds])\n",
    "    return tuple(map(int,est.transform([[pos, velocity]])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table = np.zeros(n_bins + (env.action_space.n,))\n",
    "Q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn():\n",
    "    tic = time.time()\n",
    "    reward_list = []\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    for i in range(1, 10001):\n",
    "        current, done = discretizer(*env.reset()), False\n",
    "        while not done:\n",
    "            lr = max(0.01, min(1.0, 1.0 - math.log10((i + 1) / 25)))\n",
    "            epsilon =  max(0.05, min(1, 1.0 - math.log10((i  + 1) / 25)))\n",
    "            action = np.argmax(Q_table[current])\n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            next_state = discretizer(*observation)\n",
    "            Q_table[current][action] += lr*(reward + np.max(Q_table[next_state]) - Q_table[current][action])\n",
    "            current = next_state\n",
    "            total_reward += reward\n",
    "        if i % 100 == 0:\n",
    "            t += 1\n",
    "            reward_list.append(total_reward/100)\n",
    "#            print(total_reward/100)\n",
    "            for index in range(len(reward_list)):\n",
    "                if reward_list[index] >= -110.0:\n",
    "                    toc = time.time()\n",
    "                    first_avg_r = index\n",
    "                    return f'Took {toc-tic:.2f} seconds and achieved average reward of {reward_list[first_avg_r]} in {(first_avg_r+1)*100} episodes'\n",
    "            total_reward = 0\n",
    "    toc = time.time()\n",
    "    print(f'Took {toc-tic:.2f} seconds and failed')\n",
    "    print(f'Max average reward achieved: {np.max(reward_list):.2f}')\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
